import os, sys
import math
import anndata
import numpy as np
import pandas as pd
import scanpy as sc
import scipy
import random
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.backends import cudnn
from tqdm.auto import tqdm

import matplotlib.pyplot as plt
plt.rcParams.update({'font.size': 18})

from sklearn.preprocessing import OneHotEncoder

from typing import TypeVar
A = TypeVar('anndata')  ## generic for anndata
ENC = TypeVar('OneHotEncoder')

def _set_seed(seed=1993):
    os.environ['PYTHONHASHSEED'] = str(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    cudnn.deterministic = True
    cudnn.benchmark = False
    g = torch.Generator()
    g.manual_seed(seed) # for stabilization

def _COOmtx_data_loader(mtx_prefix: str) -> A:
    '''
    Load gene score matrix in COO format
    ---
    Input:
        - mtx_prefix: gene score matrix in COO format
    ---
    Output:
        - an anndata object
    '''
    adata = anndata.read_mtx(mtx_prefix+'.mtx.gz').T
    genes = pd.read_csv(mtx_prefix+'_genes.tsv', header=None, sep='\t')
    adata.var["genes"] = genes[0].values
    adata.var_names = adata.var["genes"]
    adata.var_names_make_unique(join="-")
    adata.var.index.name = None
    cells = pd.read_csv(mtx_prefix+'_barcodes.tsv', header=None, sep='\t')
    adata.obs["barcode"] = cells[0].values
    adata.obs_names = adata.obs['barcode']
    adata.obs_names_make_unique(join="-")
    adata.obs.index.name = None

    adata = adata[:, adata.var_names.notnull()]
    adata.var_names=[i.upper() for i in list(adata.var_names)]
    adata.var_names_make_unique(join="-")
    adata.var.index.name = None
    return adata


def _csv_data_loader(csv_input: str) -> A:
    '''
    Load gene score matrix in csv format
    ---
    Input:
        - csv_input: gene scre matrix in dense format with row as genes and cells as columns.
    ---
    Output:
        - an anndata object
    '''
    df = pd.read_csv(csv_input, index_col=0)
    obs = pd.DataFrame(data=df.columns, index=df.columns)
    obs.columns = ["barcode"]
    var = pd.DataFrame(data=df.index, index=df.index)
    var.columns = ['gene_symbols']
    adata = anndata.AnnData(X=df.T, obs=obs, var=var)
    adata.obs_names_make_unique(join="-")
    adata.var_names_make_unique(join="-")

    adata = adata[:, adata.var_names.notnull()]
    adata.var_names=[i.upper() for i in list(adata.var_names)]
    return adata

def _metadata_loader(metadata):
    '''Load metadata
    '''
    metadata = pd.read_csv(metadata, index_col=0, sep=',')
    return metadata


def _process_adata(adata, process_type='train', celltype_label='celltype'):
    '''Procedures for filtering single-cell gene scale data (can be gene expression, or gene scores)
       1. Filter nonsense genes;
       2. Normalize and log-transform the data;
       3. Remove cells with no labels; 
    '''
    adata = adata[:, adata.var_names.notnull()]  ## remove NA var_names, some genes generated by ArchR gene scores will be NA
    adata.var_names=[i.upper() for i in list(adata.var_names)] #avoid some genes having lower letter

    ## make names unique after removing
    adata.var_names_make_unique()
    adata.obs_names_make_unique()

    #prefilter_specialgene: MT and ERCC  -> refered from ItClust package
    Gene1Pattern="ERCC"
    Gene2Pattern="MT-"
    id_tmp1=np.asarray([not str(name).startswith(Gene1Pattern) for name in adata.var_names],dtype=bool)
    id_tmp2=np.asarray([not str(name).startswith(Gene2Pattern) for name in adata.var_names],dtype=bool)
    id_tmp=np.logical_and(id_tmp1,id_tmp2)
    adata._inplace_subset_var(id_tmp)

    ## handel exception when there are not enough cells or genes after filtering
    if adata.shape[0] < 3 or adata.shape[1] < 3:
        sys.exit("Error: too few genes or cells left to continue..")

    ## normalization,var.genes,log1p
    sc.pp.normalize_per_cell(adata, counts_per_cell_after=10000, min_counts=0)
    sc.pp.log1p(adata)

    ## cells with celltypes
    if process_type == 'train':
        cells = adata.obs.dropna(subset=[celltype_label]).index.tolist()
        adata = adata[cells]
    return adata

def _select_feature(adata: A, model_config: dict) -> A:
    '''Select features
    ---
    Input:
        - anndata
        - fs_method: F-test / noFS / seurat
    '''
    ## Feature selection
    if model_config['fs'] == "noFS":
        print("Cellcano will not perform feature selection.\n")
        return adata
    else:
        if model_config['num_features'] > adata.shape[1]:
            print("Number of features is larger than data. Cellcano will not perform feature selection.\n")
            return adata

    if model_config['fs'] == "F-test":
        print("Use F-test to select features.\n")
        if scipy.sparse.issparse(adata.X) or \
                isinstance(adata.X, pd.DataFrame):
            tmp_data = adata.X.toarray()
        else:
            tmp_data = adata.X

        ## calculate F-test
        cell_annots = adata.obs[model_config['Celltype_COLUMN']].tolist()
        uniq_celltypes = set(cell_annots)
        array_list = []
        for celltype in uniq_celltypes:
            idx = np.where(np.array(cell_annots) == celltype)[0].tolist()
            array_list.append(tmp_data[idx, :])
        F, p = scipy.stats.f_oneway(*array_list)
        F_updated = np.nan_to_num(F)
        sorted_idx = np.argsort(F_updated)[-model_config['num_features']:]
        features = adata.var_names[sorted_idx].tolist()
        features.sort()
        adata = adata[:, features]

    if model_config['fs'] == "seurat_v3":
        print("Use seurat in scanpy to select features.\n")
        sc.pp.highly_variable_genes(adata, 
                                    n_top_genes=model_config['num_features'], 
                                    subset=True)
    return adata


def _scale_data(adata):
    '''Center scale
    '''
    adata_copy = sc.pp.scale(adata, zero_center=True, max_value=6, copy=True)
    return adata_copy

def _visualize_data(adata, output_dir, color_columns=["celltype"],
        reduction="tSNE", prefix="data", random_seed=1993):
    '''Visualize data 

    ---
    Input:
        - reduction: tSNE or UMAP
        - color_columns: plot on categories
    '''
    sc.tl.pca(adata, random_state=random_seed)

    if reduction == "tSNE":
        sc.tl.tsne(adata, use_rep="X_pca",
            learning_rate=300, perplexity=30, n_jobs=1, random_state=random_seed)
        sc.pl.tsne(adata, color=color_columns)
        plt.tight_layout()
        plt.savefig(output_dir+os.sep+prefix+"tSNE_cluster.png")
    if reduction == "UMAP":
        sc.pp.neighbors(adata, n_neighbors=20, use_rep="X_pca", random_state=random_seed) 
        sc.tl.umap(adata, random_state=random_seed)
        sc.pl.umap(adata, color=color_columns)
        plt.tight_layout()
        plt.savefig(output_dir+os.sep+prefix+"umap_cluster.png")


def _visualize_embedding(adata, output_dir, color_columns=["celltype"],
        reduction="UMAP", prefix="embedding", random_seed=1993):
    '''Visualize data 

    ---
    Input:
        - reduction: tSNE or UMAP
        - color_columns: plot on categories
    '''
    sc.tl.pca(adata, random_state=random_seed)

    if reduction == "tSNE":
        sc.tl.tsne(adata, use_rep="X_pca",
            learning_rate=300, perplexity=30, n_jobs=1, random_state=random_seed)
        sc.pl.tsne(adata, color=color_columns, ncols=1)
        plt.tight_layout()
        plt.savefig(output_dir+os.sep+prefix+"tSNE_cluster.png", bbox_inches='tight')
    if reduction == "UMAP":
        sc.pp.neighbors(adata, n_neighbors=20, use_rep="X_pca", random_state=random_seed) 
        sc.tl.umap(adata, random_state=random_seed)
        sc.pl.umap(adata, color=color_columns, ncols=1)
        plt.tight_layout()
        plt.savefig(output_dir+os.sep+prefix+"umap_cluster.png", bbox_inches='tight')


def _save_adata(adata, output_dir, prefix=""):
    '''Save anndata as h5ad
    '''
    adata.write(output_dir+os.sep+prefix+'adata.h5ad')


def _prob_to_label(y_pred: np.ndarray, encoders: dict) -> list:
    '''Turn predicted probabilites to labels
    --- 
    Input:
        - y_pred: Predicted probabilities
        - encoders: dictionary with mapping information
    ---
    Output:
        - a list containing predicted cell types
    '''
    pred_labels = y_pred.argmax(1)
    pred_celltypes = [encoders[label] for label in pred_labels]
    print("=== Predicted celltypes: ", set(pred_celltypes))
    return pred_celltypes

def _label_to_onehot(labels: list, encoders:dict) -> np.ndarray:
    '''Turn predicted labels to onehot encoder
    ---
    Input: 
        - labels: the input predicted cell types
        - encoders: dictionary with mapping information
    '''
    inv_enc = {v: k for k, v in encoders.items()}
    onehot_arr = np.zeros((len(labels), len(encoders)))
    pred_idx = [inv_enc[l] for l in labels]
    onehot_arr[np.arange(len(labels)), pred_idx] = 1
    return onehot_arr


def _extract_adata(adata: A) -> np.ndarray:
    '''Extract adata.X to a numpy array
    ---
    Output:
         - matrix in np.ndarray format
    '''
    if scipy.sparse.issparse(adata.X) or isinstance(adata.X, pd.DataFrame) or isinstance(adata.X, anndata._core.views.ArrayView):
        X = adata.X.toarray()
    else:
        X = adata.X
    return X


def _get_triplets(embeddings, labels):
    triplets = []
    labels = labels.cpu().numpy()
    for i in range(len(embeddings)):
        anchor = embeddings[i]
        anchor_label = labels[i]
        # Find positives and negatives
        pos_indices = [j for j in range(len(embeddings)) if labels[j] == anchor_label and j != i]
        neg_indices = [j for j in range(len(embeddings)) if labels[j] != anchor_label]
        if not pos_indices or not neg_indices:
            continue  # skip if can't form a triplet
        pos = embeddings[pos_indices[0]]
        neg = embeddings[neg_indices[0]]
        triplets.append((anchor, pos, neg))
    if not triplets:
        return None, None, None
    anchors, positives, negatives = zip(*triplets)
    return torch.stack(anchors), torch.stack(positives), torch.stack(negatives)

def _select_confident_cells(adata, celltype_col, entropy=0.4):
    '''Select low entropy cells from each predicted cell type
    ---
    Input:
        - adata: anndata object
        - celltype_col: the column indicator
    '''
    low_entropy_cells = []
    for celltype in set(adata.obs[celltype_col]):
        celltype_df = adata.obs[adata.obs[celltype_col] == celltype]
        entropy_cutoff = np.quantile(celltype_df['entropy'], q=entropy)
        ## change to < instead of <= to deal with ties
        cells = celltype_df.index[np.where(celltype_df['entropy'] <= entropy_cutoff)[0]].tolist()
        num_cells = math.ceil(entropy*celltype_df.shape[0])
        if len(cells) > num_cells:
            selected_cells = random.sample(cells, num_cells)
        else:
            selected_cells = cells
        low_entropy_cells.extend(selected_cells)
    high_entropy_cells = list(set(adata.obs_names) - set(low_entropy_cells))
    adata.obs.loc[low_entropy_cells, 'entropy_status'] = "low"
    adata.obs.loc[high_entropy_cells, 'entropy_status'] = "high"
    return adata

def _oversample_cells(adata, celltype_col):
    '''Oversample cell types with number of cells lower than average
    ---
    Input:
        - adata: anndata object from second round
        - celltype_col: the column indicator
    '''
    sampled_cells = []
    avg_cellnums = math.ceil(adata.shape[0]/len(set(adata.obs[celltype_col])))
    for celltype in set(adata.obs[celltype_col]):
        celltype_df = adata.obs[adata.obs[celltype_col] == celltype]
        if celltype_df.shape[0] < avg_cellnums:
            selected_cells = random.choices(list(celltype_df.index), k=avg_cellnums)
        else:
            selected_cells = list(celltype_df.index)
        sampled_cells.extend(selected_cells)
    sampled_adata = adata[sampled_cells]
    return sampled_adata.copy()
 
def _run_distiller(dataloader, student_model, teacher_model,
        epochs=30, alpha=0.1, temperature=3):
    '''Train KD model using PyTorch'''
    optimizer = optim.AdamW(student_model.parameters(), lr=3e-4) 
    teacher_model.eval()
    student_model.train()
    for epoch in tqdm(range(epochs)):
        total_loss = 0
        for batch_x, batch_y in dataloader:
            # Forward pass
            teacher_outputs = teacher_model(batch_x).detach()
            student_outputs = student_model(batch_x)
            loss = _distillation_loss(student_outputs, teacher_outputs, batch_y, alpha=alpha, T=temperature)
            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        # Print loss every 10 epochs
        if (epoch + 1) % 10 == 0:
            avg_loss = total_loss / len(dataloader)
            print(f"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}")
    return student_model

def _distillation_loss(student_outputs, teacher_outputs, labels, alpha=0.1, T=3):
    # Hard label loss
    hard_loss = F.cross_entropy(student_outputs, labels)
    # Soft label loss
    soft_teacher = F.softmax(teacher_outputs / T, dim=1)
    soft_student = F.log_softmax(student_outputs / T, dim=1)
    soft_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean')
    # Combined loss
    return alpha * hard_loss + (1 - alpha) * soft_loss


def _extract_backbone_mlp(model):
    '''Extract backbone model from batch MLP in PyTorch'''
    layers = list(model.children())[:-1]  # Remove the last layer (classifier)
    backbone_model = nn.Sequential(*layers)
    return backbone_model


